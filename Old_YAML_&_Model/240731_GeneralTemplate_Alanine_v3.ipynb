{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alanine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "import cv2\n",
    "import yaml as yml\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. YAML version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load configuration\n",
    "def load_config(version):\n",
    "    with open('config_alanine_model.yml', 'r') as file: #r is for reading \n",
    "        config = yml.safe_load(file)\n",
    "    return config['versions'][version]\n",
    "\n",
    "# Load the specific version of the configuration\n",
    "version = 11  # Change this to the desired version\n",
    "config_path = 'config_alanine_model.yml'\n",
    "config = load_config(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_na = r'C:\\Users\\PC\\Documents\\BIOSFER\\data\\alanine\\alanine_NA'\n",
    "path_normal = r'C:\\Users\\PC\\Documents\\BIOSFER\\data\\alanine\\alanine_normal'\n",
    "\n",
    "\n",
    "def load_images(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(path, filename)\n",
    "            # Read PNG with all channels\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            # Adding this correction turns the number of channels from 4 to 3, which affects the condition below!!\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is not None and img.shape == (600, 800, 3):\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                data.append((img, label, filename))\n",
    "    df = pd.DataFrame(data, columns=['Image', 'Label', 'Filename'])\n",
    "    df.set_index('Filename', inplace=True)\n",
    "\n",
    "    return images, labels, df\n",
    "\n",
    "\n",
    "# Load NA (invalid) images\n",
    "na_images, na_labels, df_0 = load_images(path_na, 0)  # 0 for invalid\n",
    "\n",
    "# Load normal (valid) images\n",
    "normal_images, normal_labels, df_1 = load_images(path_normal, 1)  # 1 for valid\n",
    "\n",
    "# Combine the data\n",
    "X = na_images + normal_images\n",
    "Y = na_labels + normal_labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Merging the two dfs\n",
    "df = pd.concat([df_0, df_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters\n",
    "coords = config['preprocessing']['coords']\n",
    "crop_size = config['preprocessing']['crop_size']\n",
    "resize_shape = tuple(config['preprocessing']['resize_shape']) #convert to tuple for cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preporcess image using the configuaration\n",
    "def preprocess_images(images, config):\n",
    "    processed_images = []\n",
    "     \n",
    "    for img in images:\n",
    "        # Crop the image\n",
    "        cropped_img = img[coords[0]:(coords[0] + crop_size[0]),\n",
    "                          coords[1]:(coords[1]+crop_size[1])]\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = cv2.resize(cropped_img, resize_shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize pixel values\n",
    "        normalized_img = resized_img.astype(np.float32) / 255.0\n",
    "\n",
    "        processed_images.append(normalized_img)\n",
    "\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images using the configuration\n",
    "X_processed = preprocess_images(df['Image'].tolist(), config)\n",
    "df['Processed'] = X_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all data to test some versions\n",
    "df_small = df.sample(n=200, random_state=42)\n",
    "df_small = pd.concat(df_small, ignore_index=True)\n",
    "df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure splits contain at least one NA sample\n",
    "def ensure_na_in_split(X, Y, na_label=0):\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "\n",
    "    # Split data into training (70%), validation (20%), and test (10%)\n",
    "    # First, split into training (70%) and temporary (30%)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "    # Second, split the temporary set into validation (20% of total) and test (10% of total)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "        X_temp, Y_temp, test_size=0.3333, random_state=42, stratify=Y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "\n",
    "# Split and ensure each set has at least one NA sample | use the df that you want\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = ensure_na_in_split(\n",
    "    df_small['Processed'], df_small['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the list of processed image arrays into a single NumPy array\n",
    "X_train_array = np.stack(X_train.values)\n",
    "X_val_array = np.stack(X_val.values)\n",
    "X_test_array = np.stack(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array, dtype=tf.float32)\n",
    "\n",
    "Y_train_tensor = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "Y_val_tensor = tf.convert_to_tensor(Y_val, dtype=tf.float32)\n",
    "Y_test_tensor = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
    "\n",
    "# Print the shapes of the arrays\n",
    "print(f'Shape of X_train_tensor: {X_train_tensor.shape}')\n",
    "print(f'Shape of X_val_tensor: {X_val_tensor.shape}')\n",
    "print(f'Shape of X_test_tensor: {X_test_tensor.shape}')\n",
    "print(f'Shape of Y_train_tensor: {Y_train_tensor.shape}')\n",
    "print(f'Shape of Y_val_tensor: {Y_val_tensor.shape}')\n",
    "print(f'Shape of Y_test_tensor: {Y_test_tensor.shape}')\n",
    "\n",
    "# Print the number of NA samples in each split\n",
    "print(f'Number of NA samples in Y_train: {np.sum(Y_train == 0)}')\n",
    "print(f'Number of NA samples in Y_val: {np.sum(Y_val == 0)}')\n",
    "print(f'Number of NA samples in Y_test: {np.sum(Y_test == 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adaptable_model(config):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.Input(shape=tuple(config['model']['input_shape'])))\n",
    "\n",
    "    # Iterate through layers defined in the YAML\n",
    "    for layer in config['model']['layers']:\n",
    "        if layer['type'] in ['Conv2D', 'SeparableConv2D']:\n",
    "            conv_layer = tf.keras.layers.Conv2D if layer['type'] == 'Conv2D' else tf.keras.layers.SeparableConv2D\n",
    "            model.add(conv_layer(\n",
    "                filters=layer['filters'],\n",
    "                kernel_size=tuple(layer['kernel_size']),\n",
    "                strides=(layer.get('strides', 1)),\n",
    "                activation=layer['activation']\n",
    "            ))\n",
    "            if 'pool_size' in layer:\n",
    "                model.add(tf.keras.layers.MaxPooling2D(pool_size=tuple(layer['pool_size'])))\n",
    "        elif layer['type'] == 'BatchNormalization':\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        elif layer['type'] == 'Flatten':\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        elif layer['type'] == 'GlobalAveragePooling2D':\n",
    "            model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        elif layer['type'] == 'Dense':\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=layer['units'],\n",
    "                activation=layer['activation']\n",
    "            ))\n",
    "            if 'dropout' in layer:\n",
    "                model.add(tf.keras.layers.Dropout(layer['dropout']))\n",
    "\n",
    "    # Handle learning rate schedule (version 10 feature)\n",
    "    if isinstance(config['training'].get('learning_rate'), dict):\n",
    "        lr_config = config['training']['learning_rate']\n",
    "        if lr_config['type'] == 'ExponentialDecay':\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=lr_config.get('initial_learning_rate', 0.001),\n",
    "                decay_steps=lr_config.get('decay_steps', 100000),\n",
    "                decay_rate=lr_config.get('decay_rate', 0.96),\n",
    "                staircase=lr_config.get('staircase', True)\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = lr_config.get('initial_learning_rate', 0.001)\n",
    "    else:\n",
    "        lr_schedule = config['training'].get('learning_rate', 0.001)\n",
    "\n",
    "    # Handle different optimizers\n",
    "    optimizer_name = config['training'].get('optimizer', 'Adam').lower()\n",
    "    optimizer_map = {\n",
    "        'adam': tf.keras.optimizers.Adam,\n",
    "        'rmsprop': tf.keras.optimizers.RMSprop,\n",
    "        'adamax': tf.keras.optimizers.Adamax,\n",
    "        'adamw': tf.keras.optimizers.AdamW\n",
    "    }\n",
    "    optimizer = optimizer_map.get(optimizer_name, tf.keras.optimizers.Adam)(learning_rate=lr_schedule)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=config['training']['loss'],\n",
    "        metrics=config['training'].get('metrics', ['accuracy'])\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = build_adaptable_model(config)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Prepare callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Handle early stopping (version 8 feature)\n",
    "if 'early_stopping' in config['training']:\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=config['training']['early_stopping'].get('monitor', 'val_loss'),\n",
    "        patience=config['training']['early_stopping'].get('patience', 5),\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    callbacks.append(early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_tensor, \n",
    "    Y_train_tensor,\n",
    "    epochs=config['training']['epochs'],\n",
    "    validation_data=(X_val_tensor, Y_val_tensor),\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n",
    "# # Train the model with class weights\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Save the model\n",
    "\n",
    "\n",
    "# model_save_path = 'C:/Users/PC/Documents/BIOSFER/trained_models/alanine_model'\n",
    "\n",
    "\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# model.save(model_save_path)\n",
    "\n",
    "\n",
    "# print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "\n",
    "\n",
    "# loaded_model = tf.keras.models.load_model('C:/Users/PC/Documents/BIOSFER/trained_models/alanine_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    X_test_tensor, Y_test_tensor, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 12. Predict and Generate Classification Report\n",
    "y_pred = model.predict(X_test_tensor)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, y_pred_classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_test_tensor)\n",
    "\n",
    "\n",
    "def plot_images_with_probabilities(series, probabilities, save_path=None):\n",
    "    # Ensure probabilities are a 1-dimensional array\n",
    "    probabilities = np.squeeze(probabilities)\n",
    "\n",
    "    # Determine the number of images\n",
    "    num_images = len(series)\n",
    "\n",
    "    # Define the grid size for plotting (adjustable based on the number of images)\n",
    "    grid_size = int(num_images**0.5)\n",
    "    if grid_size**2 < num_images:\n",
    "        grid_size += 1\n",
    "\n",
    "    # Create a figure with a grid of subplots\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "\n",
    "    # Flatten axes for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Get the image data\n",
    "        img = series.iloc[i]\n",
    "\n",
    "        # Plot the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')  # Hide axis\n",
    "\n",
    "        # Add the predictive probability as text on top of the image\n",
    "        prob_text = f\"{float(probabilities[i]) * 100:.4f}%\"\n",
    "        axes[i].text(10, 20, prob_text, color='white', fontsize=12,\n",
    "                     bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='png')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "plot_images_with_probabilities(X_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Create a Excel table to compile model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')  # or try 'TkAgg' or 'Qt5Agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor, excel_path='model_results.xlsx'):\n",
    "    # Load the specific version of the configuration\n",
    "    try:\n",
    "        config = load_config(version)\n",
    "    except KeyError:\n",
    "        print(f\"Error: Version {version} not found in the configuration file.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load existing Excel file or create a new one\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(excel_path)\n",
    "    except FileNotFoundError:\n",
    "        workbook = openpyxl.Workbook()\n",
    "\n",
    "    # Get or create the sheet for this version\n",
    "    sheet_name = f'Version {version}'\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        # Clear the existing content\n",
    "        for row in sheet[sheet.dimensions]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "    else:\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "    # 1. Model Summary\n",
    "    sheet['A1'] = 'Model Summary'\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    summary_string = \"\\n\".join(stringlist)\n",
    "    for i, line in enumerate(summary_string.split('\\n')):\n",
    "        for j, value in enumerate(line.split()):\n",
    "            sheet.cell(row=i+2, column=j+1, value=value)\n",
    "\n",
    "    # 2. Training History\n",
    "    sheet['S1'] = 'Training History'\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    for r, row in enumerate(history_df.values, start=2):\n",
    "        for c, value in enumerate(row, start=19):\n",
    "            sheet.cell(row=r, column=c, value=value)\n",
    "\n",
    "    # 3. Performance Plots\n",
    "    sheet['Y1'] = 'Performance Plots'\n",
    "\n",
    "    # Model Accuracy Plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'Y3')\n",
    "\n",
    "    # Model Loss Plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'Y27')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(X_test_tensor)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(Y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'Y51')\n",
    "\n",
    "    # 4. Image Probabilities\n",
    "    sheet['A55'] = 'Image Probabilities'\n",
    "    img_buf = BytesIO()\n",
    "    plot_images_with_probabilities(X_test, y_pred, save_path=img_buf)\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'A56')\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(excel_path)\n",
    "    print(f\"Results for version {version} compiled and saved to {excel_path}\")\n",
    "\n",
    "compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
