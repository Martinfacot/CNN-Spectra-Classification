{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #type: ignore\n",
    "import pandas as pd #type: ignore\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras.models import Model #type: ignore\n",
    "from tensorflow.keras import backend as K  #type: ignore\n",
    "import matplotlib.pyplot as plt #type: ignore\n",
    "import matplotlib # type: ignore\n",
    "import os\n",
    "import cv2 #type: ignore\n",
    "import yaml #type: ignore\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, KFold #type: ignore\n",
    "from sklearn.metrics import classification_report,accuracy_score, confusion_matrix, precision_score, recall_score, f1_score #type: ignore\n",
    "from imblearn.metrics import specificity_score #type: ignore\n",
    "import openpyxl #type: ignore\n",
    "from openpyxl.drawing.image import Image #type: ignore\n",
    "from openpyxl.utils import get_column_letter #type: ignore\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage #type: ignore\n",
    "import seaborn as sns #type: ignore\n",
    "import tempfile\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get the name of the metabolite, rename YAML file and build access paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metabolite_name():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Get the parent directory name (which should be the metabolite name)\n",
    "    metabolite_name = os.path.basename(current_dir)\n",
    "    \n",
    "    return metabolite_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files():\n",
    "    metabolite_name = get_metabolite_name().lower()  \n",
    "\n",
    "    # List files in the current directory\n",
    "    files = os.listdir(os.getcwd())\n",
    "\n",
    "    # Trouver les fichiers YAML et notebook\n",
    "    yaml_file = next((f for f in files if f.endswith('.yml') or f.endswith('.yaml')), None)\n",
    "\n",
    "    # Rename the files\n",
    "    if yaml_file:\n",
    "        new_yaml_name = f\"config_{metabolite_name}.yml\"\n",
    "        os.rename(yaml_file, new_yaml_name)\n",
    "        print(f\"Renamed {yaml_file} to {new_yaml_name}\")\n",
    "    else:\n",
    "        print(\"No YAML file found\")\n",
    "\n",
    "rename_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select model version & load YAML config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    metabolite_name = get_metabolite_name()\n",
    "    \n",
    "    version = 1 ## SELECT THE VERSION ##\n",
    "\n",
    "\n",
    "def load_config(metabolite_name, version):\n",
    "    yaml_file = f'config_{metabolite_name.lower()}.yml'\n",
    "    with open(yaml_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Convert version to integer if it's a whole number\n",
    "    version_int = int(version) if isinstance(version, float) and version.is_integer() else version\n",
    "    \n",
    "    if version_int not in config['versions']:\n",
    "        raise ValueError(f\"Version {version} not found in config file\")\n",
    "    \n",
    "    return config['versions'][version_int], str(version)\n",
    "\n",
    "config, version_str = load_config(metabolite_name, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_path(metabolite_name, path_type):\n",
    "    # Get current working directory and move up two levels\n",
    "    base_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "    print(f\"Base directory: {base_dir}\")  \n",
    "\n",
    "    if path_type in [\"abnormal\", \"normal\"]:\n",
    "        return os.path.join(base_dir, \"data\", metabolite_name, path_type)\n",
    "    elif path_type == \"models\":\n",
    "        return os.path.join(base_dir, \"models\", metabolite_name)\n",
    "    elif path_type == \"excel\":\n",
    "        return os.path.join(base_dir, f\"model_results_{metabolite_name}.xlsx\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid path type\")\n",
    "    \n",
    "path_invalid = construct_path(metabolite_name, \"abnormal\")\n",
    "path_valid = construct_path(metabolite_name, \"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(path, filename)\n",
    "            # Read PNG with all channels\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            # Adding this correction turns the number of channels from 4 to 3, which affects the condition below!!\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is not None and img.shape == (547, 671, 3): # height, width, channels\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                data.append((img, label, filename))\n",
    "    df = pd.DataFrame(data, columns=['Image', 'Label', 'Filename'])\n",
    "    df.set_index('Filename', inplace=True)\n",
    "\n",
    "    return images, labels, df\n",
    "\n",
    "\n",
    "# Load NA (invalid) images\n",
    "na_images, na_labels, df_0 = load_images(path_invalid, 0)  # 0 for invalid\n",
    "\n",
    "# Load normal (valid) images\n",
    "normal_images, normal_labels, df_1 = load_images(path_valid, 1)  # 1 for valid\n",
    "\n",
    "# Combine the data\n",
    "X = na_images + normal_images\n",
    "Y = na_labels + normal_labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Merging the two dfs\n",
    "df = pd.concat([df_0, df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better practice is to create dataframes, and have each row with its identifier, the image, and the label\n",
    "print(\"Invalid df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df_0.shape)\n",
    "print(df_0.dtypes)\n",
    "print(\" \")\n",
    "print(\"Valid df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df_1.shape)\n",
    "print(df_1.dtypes)\n",
    "print(\" \")\n",
    "print(\"Valid complete df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images with normalization and standardization\n",
    "def preprocess_images(images, coords, crop_size, resize_shape):\n",
    "    processed_images = []\n",
    "\n",
    "    for img in images:\n",
    "        # Crop the image\n",
    "        cropped_img = img[coords[0]:(\n",
    "            coords[0] + crop_size[0]), coords[1]:(coords[1]+crop_size[1])]\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = cv2.resize(\n",
    "            cropped_img, resize_shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize pixel values\n",
    "        normalized_img = resized_img.astype(np.float32) / 255.0\n",
    "\n",
    "        processed_images.append(normalized_img)\n",
    "\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = config['preprocess']['coords']\n",
    "crop_size = config['preprocess']['crop_size']\n",
    "resize_shape = tuple(config['preprocess']['resize_shape'])\n",
    "\n",
    "\n",
    "X_processed = preprocess_images(\n",
    "    df['Image'].tolist(), coords, crop_size, resize_shape)\n",
    "\n",
    "df['Processed'] = X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionne une image prétraitée à afficher\n",
    "index = 1  # Vous pouvez changer l'index pour afficher une autre image\n",
    "preprocessed_image = df['Processed'][index]\n",
    "\n",
    "# Affichage de l'image prétraitée\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(preprocessed_image)\n",
    "plt.title(f\"Procesed - Index {index}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure splits contain at least one NA sample\n",
    "def ensure_na_in_split(X, Y, na_label=0):\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(24)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "\n",
    "    # Split data into training (80%), validation (15%), and test (5%)\n",
    "    # First, split into training (80%) and temporary (20%)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "    # Second, split the temporary set into validation (15% of total) and test (5% of total)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "        X_temp, Y_temp, test_size=0.25, random_state=42, stratify=Y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "# Split and ensure each set has at least one NA sample\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = ensure_na_in_split(\n",
    "    df['Processed'], df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.stack(X_train.values)\n",
    "X_val_array = np.stack(X_val.values)\n",
    "X_test_array = np.stack(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array, dtype=tf.float32)\n",
    "\n",
    "Y_train_tensor = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "Y_val_tensor = tf.convert_to_tensor(Y_val, dtype=tf.float32)\n",
    "Y_test_tensor = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
    "\n",
    "# Print the shapes of the arrays\n",
    "print(f'Shape of X_train_tensor: {X_train_tensor.shape}')\n",
    "print(f'Shape of X_val_tensor: {X_val_tensor.shape}')\n",
    "print(f'Shape of X_test_tensor: {X_test_tensor.shape}')\n",
    "print(f'Shape of Y_train_tensor: {Y_train_tensor.shape}')\n",
    "print(f'Shape of Y_val_tensor: {Y_val_tensor.shape}')\n",
    "print(f'Shape of Y_test_tensor: {Y_test_tensor.shape}')\n",
    "\n",
    "# Print the number of NA samples in each split\n",
    "print(f'Number of NA samples in Y_train: {np.sum(Y_train == 0)}')\n",
    "print(f'Number of NA samples in Y_val: {np.sum(Y_val == 0)}')\n",
    "print(f'Number of NA samples in Y_test: {np.sum(Y_test == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching Filenames for Validation and Test Sets Based on Processed Images\n",
    "\n",
    "# Ensure 'Processed' column contains preprocessed images\n",
    "df_all = df.reset_index()\n",
    "df_all['Processed'] = df_all['Image'].apply(\n",
    "    lambda img: preprocess_images([img], coords, crop_size, resize_shape)[0]\n",
    ")\n",
    "\n",
    "def find_filenames_by_label(X_set, Y_set, label, df_all):\n",
    "    \"\"\"\n",
    "    Finds filenames in a given dataset for a specific label.\n",
    "    \n",
    "    Args:\n",
    "        X_set (np.ndarray): Array of images (e.g., X_val or X_test).\n",
    "        Y_set (np.ndarray): Array of labels corresponding to X_set.\n",
    "        label (int): The target label to filter.\n",
    "        df_all (pd.DataFrame): DataFrame containing 'Processed' images and 'Filename'.\n",
    "\n",
    "    Returns:\n",
    "        list: Filenames corresponding to the target label.\n",
    "    \"\"\"\n",
    "    # Find matching filenames\n",
    "    filenames = [\n",
    "        df_all.loc[\n",
    "            df_all['Processed'].apply(lambda x: np.array_equal(x, img)).idxmax(),\n",
    "            'Filename'\n",
    "        ]\n",
    "        for img in X_set\n",
    "    ]\n",
    "    # Filter filenames by label\n",
    "    return [filenames[i] for i in np.where(Y_set == label)[0]]\n",
    "\n",
    "# Validation set\n",
    "label_0_val_filenames = find_filenames_by_label(X_val, Y_val, label=0, df_all=df_all)\n",
    "print(\"Validation set filenames with label 0:\")\n",
    "print(\"\\n\".join(label_0_val_filenames))\n",
    "print(f\"\\nTotal number of validation samples with label 0: {len(label_0_val_filenames)}\")\n",
    "\n",
    "# Test set\n",
    "label_0_test_filenames = find_filenames_by_label(X_test, Y_test, label=0, df_all=df_all)\n",
    "print(\"Test set filenames with label 0:\")\n",
    "print(\"\\n\".join(label_0_test_filenames))\n",
    "print(f\"\\nTotal number of test samples with label 0: {len(label_0_test_filenames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom F1 Score metric to evaluate precision and recall balance\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        self.true_positives.assign_add(tf.reduce_sum(y_pred * y_true))\n",
    "        self.false_positives.assign_add(tf.reduce_sum(y_pred * (1 - y_true)))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum((1 - y_pred) * y_true))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n",
    "        f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "        return f1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.false_positives.assign(0.)\n",
    "        self.false_negatives.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "exec(config['code'])\n",
    "model.summary()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(config['training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning : each images need to have a different name\n",
    "\n",
    "def display_image_filtered(name_image, model, layer_name, image):\n",
    "    inp = model.inputs\n",
    "    out1 = model.get_layer(layer_name).output\n",
    "    feature_map_1 = tf.keras.Model(inputs=inp, outputs=out1)\n",
    "    \n",
    "    # The image is already processed, so we just need to expand dimensions\n",
    "    input_img = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    f = feature_map_1.predict(input_img)\n",
    "    dim = f.shape[3]\n",
    "    \n",
    "    print(f'{layer_name} | Features Shape: {f.shape}')\n",
    "    print(f'Dimension {dim}')\n",
    "    \n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    if not os.path.exists(f'results_{name_image}'):\n",
    "        os.makedirs(f'results_{name_image}')\n",
    "    \n",
    "    for i in range(dim):\n",
    "        ax = fig.add_subplot(dim//2, dim//2 + dim%2, i+1)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(f[0, :, :, i])\n",
    "        plt.imsave(f'results_{name_image}/{name_image}_{layer_name}_{i}.jpg', f[0, :, :, i])\n",
    "    plt.show()\n",
    "\n",
    "# Extract layer names dynamically\n",
    "conv_layers = [layer.name for layer in model.layers if 'conv2d' in layer.name or 'max_pooling2d' in layer.name] # type: ignore\n",
    "\n",
    "# Display activation maps for specified number of images\n",
    "num_images_to_visualize = 1\n",
    "\n",
    "for i in range(num_images_to_visualize):\n",
    "    # Get both processed and original images\n",
    "    processed_image = X_train_tensor[i]\n",
    "    original_index = df[df['Processed'].apply(lambda x: np.array_equal(x, processed_image))].index[0]\n",
    "    original_image = df.loc[original_index, 'Image']\n",
    "    \n",
    "    # Display original and processed images side by side for comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(original_image)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(processed_image)\n",
    "    ax2.set_title('Processed Image')\n",
    "    ax2.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display activation maps for each layer using the processed image\n",
    "    for layer in conv_layers:\n",
    "        display_image_filtered(f'train_image_{i}', model, layer, processed_image) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_filter(model, layer_name):\n",
    "    # Get layer weights\n",
    "    layer = model.get_layer(layer_name)\n",
    "    filter, bias = layer.get_weights()\n",
    "    dim = filter.shape[3]\n",
    "    \n",
    "    print(f'{layer_name} | Filter Shape: {filter.shape} Bias Shape: {bias.shape}')\n",
    "    print(f'Dimension {dim}')\n",
    "    \n",
    "    # Normalize filter values\n",
    "    f_min, f_max = filter.min(), filter.max()\n",
    "    filter = (filter - f_min) / (f_max - f_min)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    rows = dim // 2\n",
    "    cols = dim // 2 + dim % 2\n",
    "    \n",
    "    # Create figure with proper size and spacing\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    for i in range(dim):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        # Get the filter slice and handle dimensionality\n",
    "        filter_slice = filter[:, :, :, i]\n",
    "        \n",
    "        # For first layer (RGB)\n",
    "        if filter_slice.shape[-1] == 3:\n",
    "            display_data = filter_slice\n",
    "        # For subsequent layers (single channel)\n",
    "        else:\n",
    "            # Take the first channel if multiple channels exist\n",
    "            if len(filter_slice.shape) == 3:\n",
    "                display_data = filter_slice[:, :, 0]\n",
    "            else:\n",
    "                display_data = filter_slice\n",
    "                \n",
    "        # Display filter\n",
    "        im = ax.imshow(display_data, cmap='viridis')\n",
    "        \n",
    "        # Create colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = plt.colorbar(im, cax=cax)\n",
    "        cbar.set_label('Pixels values')\n",
    "        \n",
    "        # Add title with value range\n",
    "        ax.set_title(f'Filter {i+1}\\nRange: [{display_data.min():.2f}, {display_data.max():.2f}]')\n",
    "        \n",
    "        # Remove axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f'Filters from layer: {layer_name}', fontsize=16, y=0.95)\n",
    "    plt.show()\n",
    "\n",
    "# Display filters for each convolutional layer\n",
    "def display_all_filters(model):\n",
    "    conv_layers = [layer for layer in model.layers if 'conv2d' in layer.name]\n",
    "    for layer in conv_layers:\n",
    "        display_filter(model, layer.name)\n",
    "\n",
    "display_all_filters(model) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Plot Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload this cell if you have a problem to display the images in this section\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "# Save path\n",
    "output_dir = construct_path(metabolite_name, \"models\")\n",
    "output_save_path = os.path.join(output_dir)\n",
    "\n",
    "y_pred = model.predict(X_test_tensor)  # type: ignore\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).reshape(-1)\n",
    "y_test = Y_test_tensor  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test_tensor, Y_test_tensor, verbose=2)  # type: ignore\n",
    "test_loss, test_accuracy, test_f1_score = results[0], results[1], results[2]\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_score:.4f}\")\n",
    "\n",
    "# Predict and Generate Classification Report\n",
    "# y_pred = model.predict(X_test_tensor)  # type: ignore\n",
    "# y_pred_classes = (y_pred > 0.5).astype(int).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Generates a bar plot of accuracy, precision, recall (sensitivity), specificity, and F1 score based on the predicted labels and the actual labels.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred (array-like): An array object containing the predicted labels.\n",
    "        y_test (array-like): An array object containing the actual labels.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')  # Sensitivity\n",
    "    specificity = specificity_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall (Sensitivity)': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), hue=list(metrics.keys()), palette=\"viridis\",\n",
    "                     legend=False)\n",
    "\n",
    "    # Add the numbers above the bars\n",
    "    for i, metric in enumerate(metrics.keys()):\n",
    "        ax.text(i, metrics[metric] + 0.01, f'{metrics[metric]:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.title('Test dataset metrics', size=18, fontweight='bold')\n",
    "    plt.ylabel('Value')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Make the bars thinner\n",
    "    for patch in ax.patches:\n",
    "        current_width = patch.get_width()\n",
    "        diff = current_width - 0.5\n",
    "        patch.set_width(0.5)\n",
    "        patch.set_x(patch.get_x() + diff * .5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(y_pred_classes, y_test)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_binary_labels(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Ensures that both predicted and actual labels are binary (0 or 1).\n",
    "    \n",
    "    Parameters:\n",
    "        y_pred: Array-like of predicted values\n",
    "        y_test: Array-like of actual values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (y_pred_binary, y_test_binary)\n",
    "    \"\"\"\n",
    "    def to_binary(y):\n",
    "        if isinstance(y, (np.ndarray, pd.Series)):\n",
    "            # If probabilities, convert to binary\n",
    "            if y.dtype in [np.float32, np.float64]:\n",
    "                return (y > 0.5).astype(int)\n",
    "            # If already binary-like\n",
    "            return y.astype(int)\n",
    "        return np.array(y, dtype=int)\n",
    "    \n",
    "    return to_binary(y_pred), to_binary(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(y_pred, y_test, labels, return_buffer=False):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of the confusion matrix based on the predicted labels and actual labels.\n",
    "    \n",
    "    Parameters:\n",
    "        y_pred: Array-like of predicted values\n",
    "        y_test: Array-like of actual values\n",
    "        labels: List of label names for the confusion matrix\n",
    "        return_buffer: If True, returns a BytesIO buffer containing the plot\n",
    "    \n",
    "    Returns:\n",
    "        BytesIO if return_buffer is True, None otherwise\n",
    "    \"\"\"\n",
    "    # Convert labels to binary format\n",
    "    y_pred_binary, y_test_binary = ensure_binary_labels(y_pred, y_test)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "    \n",
    "    # Plot confusion matrix heatmap\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', alpha=0.7, linewidths=2, \n",
    "                xticklabels=labels, yticklabels=labels, cmap='viridis')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Heatmap of the Test dataset Confusion Matrix', \n",
    "              fontsize=18, fontweight='bold')\n",
    "\n",
    "    if return_buffer:\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')\n",
    "        img_buf.seek(0)\n",
    "        plt.close()\n",
    "        return img_buf\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return None\n",
    "\n",
    "# Define the labels\n",
    "labels = ['Invalid', 'Valid'] \n",
    "plot_conf_matrix(y_pred_classes, y_test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### History results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, return_buffer=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate plots to visualize the training and validation metrics over epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        history: A dictionary containing the training history (typically from model.fit())\n",
    "        return_buffer (bool): If True, returns a BytesIO buffer containing the plot\n",
    "        save_path (str, optional): Path to save the plot (e.g., 'training_history.png')\n",
    "    \n",
    "    Returns:\n",
    "        BytesIO if return_buffer is True, None otherwise\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Create epochs list\n",
    "    epochs = [ep + 1 for ep in history.epoch]\n",
    "    \n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Color scheme\n",
    "    train_color = '#440154'\n",
    "    val_color = '#5ec962'\n",
    "    arrow_color = 'black'\n",
    "    \n",
    "    # Helper function for annotations\n",
    "    def add_annotation(ax, x, y, text, offset_y):\n",
    "        ax.annotate(text,\n",
    "                   xy=(x, y),\n",
    "                   xytext=(10, offset_y),\n",
    "                   textcoords='offset points',\n",
    "                   arrowprops=dict(arrowstyle='->', color=arrow_color),\n",
    "                   bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    ax1.plot(epochs, history.history['accuracy'], color=train_color, label='Train')\n",
    "    ax1.plot(epochs, history.history['val_accuracy'], color=val_color, label='Validation')\n",
    "    ax1.set_title('Model Accuracy', size=18, fontweight='bold', pad=15)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(loc='lower right')\n",
    "    \n",
    "    # Accuracy annotations\n",
    "    max_acc = max(history.history['accuracy'])\n",
    "    max_val_acc = max(history.history['val_accuracy'])\n",
    "    add_annotation(ax1, history.history['accuracy'].index(max_acc) + 1, max_acc,\n",
    "                  f'Max Train: {max_acc:.3f}\\n(Epoch {history.history[\"accuracy\"].index(max_acc) + 1})', -60)\n",
    "    add_annotation(ax1, history.history['val_accuracy'].index(max_val_acc) + 1, max_val_acc,\n",
    "                  f'Max Val: {max_val_acc:.3f}\\n(Epoch {history.history[\"val_accuracy\"].index(max_val_acc) + 1})', -80)\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    ax2.plot(epochs, history.history['f1_score'], color=train_color, label='Train')\n",
    "    ax2.plot(epochs, history.history['val_f1_score'], color=val_color, label='Validation')\n",
    "    ax2.set_title('Model F1 Score', size=18, fontweight='bold', pad=15)\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(loc='lower right')\n",
    "    \n",
    "    # F1 Score annotations\n",
    "    max_f1 = max(history.history['f1_score'])\n",
    "    max_val_f1 = max(history.history['val_f1_score'])\n",
    "    add_annotation(ax2, history.history['f1_score'].index(max_f1) + 1, max_f1,\n",
    "                  f'Max Train: {max_f1:.3f}\\n(Epoch {history.history[\"f1_score\"].index(max_f1) + 1})', -60)\n",
    "    add_annotation(ax2, history.history['val_f1_score'].index(max_val_f1) + 1, max_val_f1,\n",
    "                  f'Max Val: {max_val_f1:.3f}\\n(Epoch {history.history[\"val_f1_score\"].index(max_val_f1) + 1})', -80)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    ax3.plot(epochs, history.history['loss'], color=train_color, label='Train')\n",
    "    ax3.plot(epochs, history.history['val_loss'], color=val_color, label='Validation')\n",
    "    ax3.set_title('Model Loss', size=18, fontweight='bold', pad=15)\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.legend(loc='upper right')\n",
    "    \n",
    "    # Loss annotations\n",
    "    min_loss = min(history.history['loss'])\n",
    "    min_val_loss = min(history.history['val_loss'])\n",
    "    add_annotation(ax3, history.history['loss'].index(min_loss) + 1, min_loss,\n",
    "                  f'Min Train: {min_loss:.3f}\\n(Epoch {history.history[\"loss\"].index(min_loss) + 1})', 40)\n",
    "    add_annotation(ax3, history.history['val_loss'].index(min_val_loss) + 1, min_val_loss,\n",
    "                  f'Min Val: {min_val_loss:.3f}\\n(Epoch {history.history[\"val_loss\"].index(min_val_loss) + 1})', 60)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Handle output\n",
    "    if save_path:\n",
    "        filename_history_results = f'History_{metabolite_name}_v{version_str}.png'\n",
    "        plt.savefig(os.path.join(output_dir, filename_history_results), dpi=300)\n",
    "        print(f\"Plot saved as: History_{metabolite_name}_v{version_str}.png in {output_save_path}\")\n",
    "            \n",
    "    if return_buffer:\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')\n",
    "        img_buf.seek(0)\n",
    "        plt.close()  # Close the figure to free memory\n",
    "        return img_buf\n",
    "    \n",
    "    elif plt.isinteractive():\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "plot_history(history) # type: ignore\n",
    "\n",
    "# Example usage:\n",
    "# plot_history(history)  # Simple display\n",
    "# plot_history(history, save_path=True)  # Save to file + display\n",
    "# buffer = plot_history(history, return_buffer=True)  # Get buffer + display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability output with associated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning : each images need to have a different name\n",
    "def plot_images_with_probabilities(df, probabilities, save_path=None):\n",
    "    probabilities = np.squeeze(probabilities)\n",
    "    num_images = len(df)\n",
    "    grid_size = int(num_images**0.5)\n",
    "    if grid_size**2 < num_images:\n",
    "        grid_size += 1\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(100, 100))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        img = row['Image']\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "        # Determine the color based on the probability\n",
    "        probability = float(probabilities[i])\n",
    "        if probability >= 0.995:\n",
    "            color = 'green'\n",
    "        elif 0.05 < probability < 0.995:\n",
    "            color = 'orange'\n",
    "        else:\n",
    "            color = 'red'\n",
    "\n",
    "        prob_text = f\"{probability * 100:.4f}%\"\n",
    "        axes[i].text(10, 20, prob_text, color='white', fontsize=60,\n",
    "                     bbox=dict(facecolor=color, alpha=0.5))\n",
    "\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # If save_path is provided, save the figure first\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=100)\n",
    "    \n",
    "    # Show the plot after saving\n",
    "    plt.show()\n",
    "    \n",
    "    # Close the figure to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "# Usage example:\n",
    "# Assuming X_test contains the filenames of the test set images\n",
    "X_test_filenames = X_test.index if hasattr(X_test, 'index') else X_test\n",
    "\n",
    "# Create a DataFrame for the test set\n",
    "df_test = df.loc[X_test_filenames]\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_test_tensor) # type: ignore\n",
    "\n",
    "# Create the output filename\n",
    "filename_probability_output = f'probability_output_{metabolite_name}_v{version_str}.png'\n",
    "output_path = os.path.join(output_dir, filename_probability_output)\n",
    "\n",
    "# Plot and save in one go\n",
    "plot_images_with_probabilities(df_test, y_pred_prob, save_path=output_path)\n",
    "\n",
    "print(f\"Plot saved as: {filename_probability_output} in {output_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Excel table compile model result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor, \n",
    "                                 metabolite_name, y_pred_classes, excel_path=None):\n",
    "    if excel_path is None:\n",
    "        excel_path = f'models_results_{metabolite_name}.xlsx'\n",
    "    matplotlib.use('Agg')\n",
    "     \n",
    "    # Load the specific version of the configuration\n",
    "    try:\n",
    "        config, version_str = load_config(metabolite_name, version)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load existing Excel file or create a new one\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(excel_path)\n",
    "    except FileNotFoundError:\n",
    "        workbook = openpyxl.Workbook()\n",
    "\n",
    "    # Get or create the sheet for this version\n",
    "    sheet_name = f'Version {version}'\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        # Clear the existing content\n",
    "        for row in sheet[sheet.dimensions]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "    else:\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "    # 1. Model Summary\n",
    "    sheet['A1'] = 'Model Summary'\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    summary_string = \"\\n\".join(stringlist)\n",
    "\n",
    "    # Save the summary to a temporary text file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8', suffix='.txt') as tmpfile:\n",
    "        tmpfile.write(summary_string)\n",
    "        tmpfile_path = tmpfile.name\n",
    "\n",
    "    # Convert the text file to an image\n",
    "    img_buf = BytesIO()\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.text(0.01, 0.99, summary_string, va='top', ha='left', wrap=True, fontsize=10, family='monospace')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(img_buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "\n",
    "    # Insert the image into the Excel sheet\n",
    "    img = Image(img_buf) # type: ignore\n",
    "    sheet.add_image(img, 'A2')\n",
    "        \n",
    "    # 2. Training History\n",
    "    sheet['M1'] = 'Training History'\n",
    "    sheet['M2'] = 'Accuracy'\n",
    "    sheet['N2'] = 'f1_score'\n",
    "    sheet['O2'] = 'Loss'\n",
    "    sheet['P2'] = 'Val Accuracy'\n",
    "    sheet['Q2'] = 'Val_f1_score'\n",
    "    sheet['R2'] = 'Val Loss'\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    for r, row in enumerate(history_df.values, start=3):\n",
    "        for c, value in enumerate(row, start=13):\n",
    "            sheet.cell(row=r, column=c, value=value)\n",
    "\n",
    "    # 3. Performance Plots - Using plot_history function\n",
    "    img_buf = plot_history(history, return_buffer=True)\n",
    "    # Add the image to the Excel sheet\n",
    "    img = Image(img_buf) # type: ignore\n",
    "    img.width, img.height = img.width // 3, img.height // 3  # Reduce image size\n",
    "    sheet.add_image(img, 'T1')\n",
    "\n",
    "    # 4. Confusion Matrix\n",
    "    labels = ['Invalid', 'Valid']\n",
    "    img_buf = plot_conf_matrix(y_pred, y_test, labels, return_buffer=True)\n",
    "    \n",
    "    # Convert buffer to image\n",
    "    img = Image(img_buf)\n",
    "    # Adjust image size (you can modify these values)\n",
    "    img.width = 600\n",
    "    img.height = 600\n",
    "    \n",
    "    # Add the image to the sheet \n",
    "    sheet.add_image(img, 'K40')\n",
    "\n",
    "    # 5. Model Code\n",
    "    sheet['A35'] = 'Model Code'\n",
    "    model_code = config.get('code', 'Model code not available') if isinstance(config, dict) else 'Model code not available'\n",
    "    model_code_lines = model_code.split('\\n')\n",
    "    for i, line in enumerate(model_code_lines, start=36):\n",
    "        sheet.cell(row=i, column=1, value=line)\n",
    "\n",
    "    # 6. Training Code\n",
    "    sheet['A70'] = 'Training Code'\n",
    "    training_code = config.get('training', 'Training code not available') if isinstance(config, dict) else 'Training code not available'\n",
    "    training_code_lines = training_code.split('\\n')\n",
    "    for i, line in enumerate(training_code_lines, start=71):\n",
    "        sheet.cell(row=i, column=1, value=line)\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(excel_path)\n",
    "    print(f\"Results for version {version} compiled and saved to {output_save_path} as {excel_path}\")\n",
    "\n",
    "\n",
    "compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor, # type: ignore\n",
    "                                 metabolite_name, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_dir = construct_path(metabolite_name, \"models\")\n",
    "model_filename = f'model_{metabolite_name}_v{version_str}.keras'\n",
    "model_save_path = os.path.join(model_save_dir, model_filename)\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "model.save(model_save_path)  # type: ignore\n",
    "\n",
    "print(f\"Model saved as:{model_filename} in {model_save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
