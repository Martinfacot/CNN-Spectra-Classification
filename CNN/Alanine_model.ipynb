{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alanine Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils import get_column_letter\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "import seaborn as sns\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path_na = r'c:\\Users\\PC\\Documents\\BIOSFER\\data\\alanine\\alanine_NA'\n",
    "path_normal = r'c:\\Users\\PC\\Documents\\BIOSFER\\data\\alanine\\alanine_normal'\n",
    "\n",
    "\n",
    "def load_images(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(path, filename)\n",
    "            # Read PNG with all channels\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            # Adding this correction turns the number of channels from 4 to 3, which affects the condition below!!\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is not None and img.shape == (600, 800, 3):\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                data.append((img, label, filename))\n",
    "    df = pd.DataFrame(data, columns=['Image', 'Label', 'Filename'])\n",
    "    df.set_index('Filename', inplace=True)\n",
    "\n",
    "    return images, labels, df\n",
    "\n",
    "\n",
    "# Load NA (invalid) images\n",
    "na_images, na_labels, df_0 = load_images(path_na, 0)  # 0 for invalid\n",
    "\n",
    "# Load normal (valid) images\n",
    "normal_images, normal_labels, df_1 = load_images(path_normal, 1)  # 1 for valid\n",
    "\n",
    "# Combine the data\n",
    "X = na_images + normal_images\n",
    "Y = na_labels + normal_labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Merging the two dfs\n",
    "df = pd.concat([df_0, df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better practice is to create dataframes, and have each row with its identifier, the image, and the label\n",
    "print(\"Invalid df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df_0.shape)\n",
    "print(df_0.dtypes)\n",
    "print(\" \")\n",
    "print(\"Valid df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df_1.shape)\n",
    "print(df_1.dtypes)\n",
    "print(\" \")\n",
    "print(\"Valid complete df description\")\n",
    "print(\"----------------------------------\")\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load YAML config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(version):\n",
    "    with open('config_pol.yml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config['versions'][version]\n",
    "\n",
    "version = 7  # Change this to the desired version\n",
    "config = load_config(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images with normalization and standardization\n",
    "def preprocess_images(images, coords, crop_size, resize_shape):\n",
    "    processed_images = []\n",
    "\n",
    "    for img in images:\n",
    "        # Crop the image\n",
    "        cropped_img = img[coords[0]:(\n",
    "            coords[0] + crop_size[0]), coords[1]:(coords[1]+crop_size[1])]\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = cv2.resize(\n",
    "            cropped_img, resize_shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize pixel values\n",
    "        normalized_img = resized_img.astype(np.float32) / 255.0\n",
    "\n",
    "        processed_images.append(normalized_img)\n",
    "\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = config['preprocess']['coords']\n",
    "crop_size = config['preprocess']['crop_size']\n",
    "resize_shape = tuple(config['preprocess']['resize_shape'])\n",
    "\n",
    "\n",
    "X_processed = preprocess_images(\n",
    "    df['Image'].tolist(), coords, crop_size, resize_shape)\n",
    "\n",
    "df['Processed'] = X_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Display 2 images, original and processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to fix appropriate coordinated\n",
    "def display_images(img1, img2):\n",
    "    matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "    axes[0].imshow(img1)\n",
    "\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(img2)\n",
    "\n",
    "    axes[1].axis('off')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(df.iloc[378]['Image'],\n",
    "               df.iloc[378]['Processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(n=200, random_state=42)\n",
    "df_small = pd.concat(df_small, ignore_index=True)\n",
    "df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure splits contain at least one NA sample\n",
    "def ensure_na_in_split(X, Y, na_label=0):\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "\n",
    "    # Split data into training (70%), validation (20%), and test (10%)\n",
    "    # First, split into training (70%) and temporary (30%)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "    # Second, split the temporary set into validation (20% of total) and test (10% of total)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "        X_temp, Y_temp, test_size=0.3333, random_state=42, stratify=Y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "# Split and ensure each set has at least one NA sample\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = ensure_na_in_split(\n",
    "    df['Processed'], df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.stack(X_train.values)\n",
    "X_val_array = np.stack(X_val.values)\n",
    "X_test_array = np.stack(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array, dtype=tf.float32)\n",
    "\n",
    "Y_train_tensor = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "Y_val_tensor = tf.convert_to_tensor(Y_val, dtype=tf.float32)\n",
    "Y_test_tensor = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
    "\n",
    "# Print the shapes of the arrays\n",
    "print(f'Shape of X_train_tensor: {X_train_tensor.shape}')\n",
    "print(f'Shape of X_val_tensor: {X_val_tensor.shape}')\n",
    "print(f'Shape of X_test_tensor: {X_test_tensor.shape}')\n",
    "print(f'Shape of Y_train_tensor: {Y_train_tensor.shape}')\n",
    "print(f'Shape of Y_val_tensor: {Y_val_tensor.shape}')\n",
    "print(f'Shape of Y_test_tensor: {Y_test_tensor.shape}')\n",
    "\n",
    "# Print the number of NA samples in each split\n",
    "print(f'Number of NA samples in Y_train: {np.sum(Y_train == 0)}')\n",
    "print(f'Number of NA samples in Y_val: {np.sum(Y_val == 0)}')\n",
    "print(f'Number of NA samples in Y_test: {np.sum(Y_test == 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(config['code'])\n",
    "model.summary()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(config['training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "# Plot Performance\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'],label='Training Accuracy')  # type: ignore\n",
    "plt.plot(history.history['val_accuracy'],label='Validation Accuracy')  # type: ignore\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')  # type: ignore\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')  # type: ignore\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(  # type: ignore\n",
    "    X_test_tensor, Y_test_tensor, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 12. Predict and Generate Classification Report\n",
    "# ================================================\n",
    "# ================================================\n",
    "y_pred = model.predict(X_test_tensor)  # type: ignore\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).reshape(-1)\n",
    "# ================================================\n",
    "# ================================================\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_probabilities(series, probabilities, save_path=None):\n",
    "    probabilities = np.squeeze(probabilities)\n",
    "    num_images = len(series)\n",
    "    grid_size = int(num_images**0.5)\n",
    "    if grid_size**2 < num_images:\n",
    "        grid_size += 1\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(100, 100))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        img = series.iloc[i]\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')  # Hide axis\n",
    "\n",
    "        # Determine the color based on the probability\n",
    "        probability = float(probabilities[i])\n",
    "        if probability >= 0.995:\n",
    "            color = 'green'\n",
    "        elif 0.05 < probability < 0.995:\n",
    "            color = 'orange'\n",
    "        else:\n",
    "            color = 'red'\n",
    "\n",
    "        prob_text = f\"{probability * 100:.4f}%\"\n",
    "        axes[i].text(10, 20, prob_text, color='white', fontsize=12,\n",
    "                     bbox=dict(facecolor=color, alpha=0.5))\n",
    "\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_test_tensor)  # type: ignore\n",
    "plot_images_with_probabilities(X_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Create a Excel table to compile model result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor,\n",
    "                                   excel_path='model_results.xlsx'):\n",
    "    matplotlib.use('Agg')  # or try 'TkAgg' or 'Qt5Agg'\n",
    "    # Load the specific version of the configuration\n",
    "    try:\n",
    "        config = load_config(version)\n",
    "    except KeyError:\n",
    "        print(f\"Error: Version {version} not found in the configuration file.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load existing Excel file or create a new one\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(excel_path)\n",
    "    except FileNotFoundError:\n",
    "        workbook = openpyxl.Workbook()\n",
    "\n",
    "    # Get or create the sheet for this version\n",
    "    sheet_name = f'Version {version}'\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        # Clear the existing content\n",
    "        for row in sheet[sheet.dimensions]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "    else:\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "    # 1. Model Summary\n",
    "    sheet['A1'] = 'Model Summary'\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    summary_string = \"\\n\".join(stringlist)\n",
    "\n",
    "    # Save the summary to a temporary text file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8', suffix='.txt') as tmpfile:\n",
    "        tmpfile.write(summary_string)\n",
    "        tmpfile_path = tmpfile.name\n",
    "\n",
    "    # Convert the text file to an image\n",
    "    img_buf = BytesIO()\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.text(0.01, 0.99, summary_string, va='top', ha='left', wrap=True, fontsize=10, family='monospace')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(img_buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "\n",
    "    # Insert the image into the Excel sheet\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'A2')\n",
    "        \n",
    "    # 2. Training History\n",
    "    sheet['M1'] = 'Training History'\n",
    "    sheet['M2'] = 'Accuracy'\n",
    "    sheet['N2'] = 'Loss'\n",
    "    sheet['O2'] = 'Val Accuracy'\n",
    "    sheet['P2'] = 'Val Loss'\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    for r, row in enumerate(history_df.values, start=3):\n",
    "        for c, value in enumerate(row, start=13):\n",
    "            sheet.cell(row=r, column=c, value=value)\n",
    "\n",
    "    # 3. Performance Plots\n",
    "\n",
    "    # Model Accuracy Plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'R1')\n",
    "\n",
    "    # Model Loss Plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'R22')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(X_test_tensor)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(Y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    img_buf = BytesIO()\n",
    "    plt.savefig(img_buf, format='png')\n",
    "    plt.close()\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'R43')\n",
    "\n",
    "    # 4. Image Probabilities\n",
    "    sheet['AD1'] = 'Image Probabilities'\n",
    "    img_buf = BytesIO()\n",
    "    plot_images_with_probabilities(X_test, y_pred, save_path=img_buf)\n",
    "    img_buf.seek(0)\n",
    "    img = Image(img_buf)\n",
    "    sheet.add_image(img, 'AD2')\n",
    "\n",
    "    # 5. Model Code\n",
    "    sheet['A35'] = 'Model Code'\n",
    "    model_code = config['code']\n",
    "    model_code_lines = model_code.split('\\n')\n",
    "    for i, line in enumerate(model_code_lines, start=36):\n",
    "        sheet.cell(row=i, column=1, value=line)\n",
    "\n",
    "    # 6. Training Code\n",
    "    sheet['A76'] = 'Training Code'\n",
    "    training_code = config['training']\n",
    "    training_code_lines = training_code.split('\\n')\n",
    "    for i, line in enumerate(training_code_lines, start=77):\n",
    "        sheet.cell(row=i, column=1, value=line)\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(excel_path)\n",
    "    print(f\"Results for version {version} compiled and saved to {excel_path}\")\n",
    "    \n",
    "compile_model_results_to_excel(version, model, history, X_test, Y_test, X_test_tensor, Y_test_tensor)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model_save_path = 'C:/Users/PC/Documents/BIOSFER/trained_models/alanine_model'\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "# model.save(model_save_path)\n",
    "# print(f\"Model saved to {model_save_path}\")\n",
    "# Load the model\n",
    "# loaded_model = tf.keras.models.load_model('C:/Users/PC/Documents/BIOSFER/trained_models/alanine_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
